import pybullet as p
import pybullet_data
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
import os

class SimpleArmEnv(gym.Env):
    def __init__(self):
        super().__init__()
        p.connect(p.DIRECT)  # –ë–µ–∑ GUI –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        p.setAdditionalSearchPath(pybullet_data.getDataPath())

        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)

        self.success_count = 0
        self.episode_steps = 0
        self.max_episode_steps = 100

    def reset(self, seed=None):
        p.resetSimulation()
        p.setGravity(0, 0, -9.81)
        p.loadURDF("plane.urdf")
        self.robot = p.loadURDF("kuka_iiwa/model.urdf", useFixedBase=True)

        for i in range(p.getNumJoints(self.robot)):
            if i not in [0, 3]:
                p.setJointMotorControl2(self.robot, i, p.VELOCITY_CONTROL, force=0)
                p.resetJointState(self.robot, i, 0)

        # –ë–ª–∏–∂–µ —Ü–µ–ª—å = –ø—Ä–æ—â–µ –Ω–∞ —Å—Ç–∞—Ä—Ç–µ –æ–±—É—á–µ–Ω–∏—è
        self.target_pos = [0.3, 0.0, 0.3]
        vis_id = p.createVisualShape(p.GEOM_SPHERE, radius=0.05, rgbaColor=[1, 0, 0, 1])
        p.createMultiBody(0, vis_id, basePosition=self.target_pos)

        p.resetJointState(self.robot, 0, 0)
        p.resetJointState(self.robot, 3, 0)

        self.last_distance = None
        self.episode_steps = 0
        return self._get_obs(), {}

    def _get_obs(self):
        joints = [p.getJointState(self.robot, j)[0] for j in [0, 3]]
        eff = p.getLinkState(self.robot, 6)[0]
        return np.array([
            *joints,
            eff[0], eff[1],
            self.target_pos[0], self.target_pos[1]
        ])

    def step(self, action):
        self.episode_steps += 1

        scaled = action * (np.pi / 4)
        for i, j in enumerate([0, 3]):
            p.setJointMotorControl2(self.robot, j, p.POSITION_CONTROL,
                                    targetPosition=scaled[i], force=1000)

        for _ in range(5):
            p.stepSimulation()

        obs = self._get_obs()
        eff_pos = p.getLinkState(self.robot, 6)[0]
        distance = np.linalg.norm(np.array(eff_pos[:2]) - np.array(self.target_pos[:2]))

        # –ú—è–≥–∫–∏–π shaping-—Ä–µ–≤–∞—Ä–¥ —Å–æ —à—Ç—Ä–∞—Ñ–æ–º –∑–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        reward = -distance
        if self.last_distance is not None:
            reward += (self.last_distance - distance) * 30
        self.last_distance = distance

        success = (distance < 0.1)
        if success:
            reward += 50
            self.success_count += 1
            done = True
        elif self.episode_steps >= self.max_episode_steps:
            done = True
        else:
            done = False

        return obs, reward, done, False, {}

# üõ† –ù–∞—Å—Ç—Ä–æ–π–∫–∏
model_path = "models/simplified_arm"
log_dir = "logs"
os.makedirs("models", exist_ok=True)

# –ü–æ–¥–∫–ª—é—á–∞–µ–º Monitor –¥–ª—è –ª–æ–≥–æ–≤ –∏ ep_rew_mean
env = Monitor(SimpleArmEnv(), filename=None)

model = PPO(
    "MlpPolicy",
    env,
    verbose=1,
    tensorboard_log=log_dir,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99
)

print(" –û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è...")
model.learn(total_timesteps=200_000)
model.save(model_path)
print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}.zip")

# üî¢ –°–∫–æ–ª—å–∫–æ —Ü–µ–ª–µ–π –±—ã–ª–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ?
# –î–æ—Å—Ç–∞—ë–º –∏–∑ env
raw_env = env.env  # —ç—Ç–∞ —Å—Ç—Ä–æ—á–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è!
print(f"\n –í—Å–µ–≥–æ —É—Å–ø–µ—à–Ω—ã—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π —Ü–µ–ª–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ: {raw_env.success_count}")


# –û—Ç–∫–ª—é—á–∞–µ–º —Ñ–∏–∑–∏–∫—É
p.disconnect()
